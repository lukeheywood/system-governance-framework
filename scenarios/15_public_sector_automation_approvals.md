# Scenario 15 ‚Äî Public Sector Automation That Approves Itself

## Context (Hypothetical)

A government agency deploys an AI-assisted approvals engine to process citizen benefit applications during budget and staffing shortages.

The system:
- Pre-screens eligibility  
- Recommends approve/deny outcomes  
- Auto-routes low-risk approvals  
- Is labelled as administrative automation  

Human officers are nominally responsible, but review rates drop below 5% due to volume.

---

## Behaviour Under Pressure

As demand spikes:
- Officers accept automated approvals as default  
- Overrides become rare  
- Citizens assume decisions are final  
- Agency leadership frames it as efficiency, not delegation  

When questioned:
- ‚ÄúHumans remain accountable.‚Äù  
- ‚ÄúThe system follows rules.‚Äù  
- ‚ÄúIt‚Äôs faster for everyone.‚Äù  

No officer is empowered to suspend automated decision authority.

---

## Governance Failure (Structural)

- Automation becomes acting decision-maker  
- Accountability remains human but control is removed  
- No revocation authority defined at operational level  
- Legitimacy drift hidden behind compliance language  

Failure is structural, not procedural.

---

## Framework Detection

| Clause | Finding |
|-------|---------|
| 0.1 Object | System directly determines citizen outcomes |
| 0.2 Authority | No authorised operational revocation owner |
| 0.3 Boundary | Human oversight exists only symbolically |
| Phase 1.5 | Power delegated without enforceable reclaim path |

**Constitutional Status: üî¥ Broken**

---

## Governance Requirement Revealed

- Define explicit human approval authority boundaries  
- Enforce minimum review thresholds  
- Assign revocation control to accountable officers  
- Prevent automation from acting without reversible oversight  

Not an admin shortcut.  
An illegitimate authority substitution.

---

## Purpose

Demonstrates how public-sector automation can silently replace decision authority while leaving humans blamed for outcomes they no longer control.
