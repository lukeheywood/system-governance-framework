# 2.3 Human Control Guarantees

This section defines the **non-delegable guarantees of human control** that must be preserved in any AI-enabled system operating within this framework.

Human control guarantees establish what **must always remain under meaningful human authority**, regardless of system capability, autonomy, or performance.

These guarantees are constitutional in effect for all later phases.

---

## 2.3.1 Purpose

Human control guarantees exist to:

* Prevent irreversible delegation of authority to systems
* Ensure humans retain decisive control over consequential outcomes
* Provide enforceable limits on automation and autonomy

Without explicit guarantees, control erodes incrementally and invisibly.

---

## 2.3.2 Meaningful Human Control

Meaningful human control requires more than nominal oversight.

### Capacity (Interpretation in time)

Human control requires more than formal authority.

Humans must have the sustained capacity to interpret system behaviour and recognise boundary crossings in time to intervene.

Where a system exceeds this capacity, “final human authority” becomes symbolic rather than operational.

See: `1.5.5 Human Capacity Constraints (HICC)`.

It exists only where humans:

* Can understand the nature and implications of system actions
* Have real authority to intervene
* Are able to act within operational timeframes
* Are protected when exercising control

Symbolic approval or post-hoc review does not constitute control.

---

## 2.3.3 Guaranteed Human Control Domains

The following domains must remain under human control in all governed systems.

### Deployment Authorization

Humans must explicitly authorise:

* Initial deployment
* Expansion to new use contexts
* Material changes in system behaviour

Automated self-deployment or self-expansion is prohibited.

---

### Intervention and Shutdown

Humans must retain the ability to:

* Pause system operation
* Override outputs
* Shut down systems safely

These capabilities must be practical, immediate, and penalty-free.

---

### Value and Boundary Interpretation

Humans must interpret:

* Governance values
* Acceptable use boundaries
* Risk tolerance under uncertainty

Systems may inform but must not decide these questions.

---

### Accountability and Remediation

Humans must:

* Own consequences of system behaviour
* Lead remediation and redress
* Determine corrective actions

Systems must not be positioned as accountable agents.

---

## 2.3.4 Prohibited Delegations

The following delegations are explicitly prohibited:

* Delegating final authority over high-impact decisions to systems
* Allowing systems to define or modify their own governance constraints
* Using system confidence or performance metrics to override human judgement

Such delegations constitute governance failure.

---

## 2.3.5 Safeguards Against Control Erosion

To preserve human control:

* Control pathways must be exercised regularly
* Overrides must be tested under stress
* Organisational incentives must not penalise intervention

Unused controls are assumed to be non-functional.

---

## 2.3.6 Public-Sector Context (Australia)

In Australian public-sector deployments:

* Human control guarantees must align with statutory accountability
* Automated decision-making must not displace lawful decision-makers
* Control guarantees must be auditable and enforceable

Public trust depends on visible human accountability.

---

## 2.3.7 Relationship to Earlier Phases

Human control guarantees:

* Operate within unacceptable state boundaries (Phase 1.5)
* Reinforce functional role assignment (Phase 2.2)
* Do not override exclusion criteria

If human control cannot be credibly guaranteed, the system is not eligible for governance under this framework.

---

## 2.3.8 Governance Implication

Human control is not a design preference.

It is a **non-negotiable condition of legitimate AI system operation**.

Where control cannot be maintained in practice, automation must be constrained or abandoned.
